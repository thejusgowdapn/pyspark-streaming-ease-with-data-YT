{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4d9251b-2a38-49f4-818c-f451db093992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Reading from Sockets\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partition\",8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4748dcc0-dff2-4c0c-852f-6fe7c41cb23c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fe80270-3a99-4411-92fa-6e179cf107ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read input data \n",
    "df_raw = spark.readStream.format(\"socket\").option(\"host\",\"localhost\").option(\"port\",\"9999\").load(\"data/input/example.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ffb6d85-3c17-4830-b632-dc2ef41ff00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec1c1368-12d5-4fd1-8732-b72998a63996",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_raw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80de07ec-87ad-40f5-9261-95fc24ad7864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the line into words\n",
    "from pyspark.sql.functions import split\n",
    "df_words = df_raw.withColumn(\"words\",split(\"value\",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b217c8f9-985f-4318-a75b-1025388cee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e686ccb-5122-4fc4-a539-5a425dfd8853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode the list of words\n",
    "from pyspark.sql.functions import explode\n",
    "df_explode = df_words.withColumn(\"word\",explode(\"words\")).drop(\"value\",\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67951bd0-542c-48fe-b2a1-bdfd970a01eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_explode.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca0b8dcc-47c0-446f-86ec-29a0a1df8098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate the words to generate count\n",
    "from pyspark.sql.functions import count,lit\n",
    "df_agg = df_explode.groupBy(\"word\").agg(count(lit(1)).alias(\"cnt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e6d96f2-9ed2-4cad-b487-e8c24231ba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14361ccc-35ef-403f-925e-d7a116a6e186",
   "metadata": {},
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "Query [id = d0014bb9-3a12-4382-906c-119661009b2d, runId = de4fa098-34c0-4ed8-99b3-7a434c8c6a54] terminated with exception: Connection refused (Connection refused)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# write the output to console\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf_agg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconsole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/spark/python/pyspark/sql/streaming.py:107\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: Query [id = d0014bb9-3a12-4382-906c-119661009b2d, runId = de4fa098-34c0-4ed8-99b3-7a434c8c6a54] terminated with exception: Connection refused (Connection refused)"
     ]
    }
   ],
   "source": [
    "# write the output to console\n",
    "\n",
    "df_agg.writeStream.format(\"console\").outputMode(\"complete\").start().awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35a6ab0a-367a-42a3-9b31-ca802d3af7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227d94be-87e5-4251-997f-939be8ad7ea0",
   "metadata": {},
   "source": [
    "The three modes are:\n",
    "Mode\tDescription\n",
    "append\tWrites only new rows since the last trigger.\n",
    "update\tWrites only updated rows (rows that changed since last trigger).\n",
    "complete\tWrites the entire result table every time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cffc002-64e3-44ab-b4dd-eb59e6906a16",
   "metadata": {},
   "source": [
    "⚙️ Output modes in streaming\n",
    "Mode\tDescription\tWorks with Aggregation?\n",
    "append\tOnly newly added rows are written to sink\t❌ Not allowed for aggregations without watermark\n",
    "update\tOnly updated results for existing groups are written\t✅ Allowed\n",
    "complete\tEntire result table (all groups) is written\t✅ Allowed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
